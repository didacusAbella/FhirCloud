{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 95.963134765625,
      "end_time": 1597500036890.024
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-de5cff312aed>:35: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  figure = plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinishJob!\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patch\n",
    "import io\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Because we are in local environment some changes are needed\n",
    "# 1. create Spark session local\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "metadata_data_dir = \"metadata.csv\"\n",
    "volumetric_data_dir = \"images/*.pck\"\n",
    "connection_string = \"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://fhirblob:10000/devstoreaccount1\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "def toImage(mri, metadata):\n",
    "    filename = mri[0].split(\"/\").pop()\n",
    "    exam = next((x for x in metadata if x['volumeFilename'] == filename), None)\n",
    "    volumetric_data = pickle.loads(mri[1])\n",
    "    # Get all roi slices from volume\n",
    "    z_start = exam['roiZ']\n",
    "    depth = exam['roiDepth']\n",
    "    plot = None\n",
    "    for z in range(z_start, z_start + depth):\n",
    "        slice = volumetric_data[z, :, :]\n",
    "        # Get roi dimensions\n",
    "        x, y, w, h = [exam[attr] for attr in ['roiX', 'roiY', 'roiWidth', 'roiHeight']]\n",
    "        # Extract ROI\n",
    "        roi = slice[y:y+h, x:x+w]\n",
    "        # Plot slice and roi\n",
    "        plt.ioff()\n",
    "        figure = plt.figure()\n",
    "        plot = plt.subplot2grid((1, 4), (0, 0), 1, 3) # This makes the slice plot larger than roi plot\n",
    "        plot.add_patch(patch.Rectangle((x, y), w, h, fill=None, color='red'))\n",
    "        plot.imshow(slice, cmap='gray')\n",
    "        plot = plt.subplot2grid((1, 4), (0, 3), 1, 1)\n",
    "        plot.imshow(roi, cmap='gray')\n",
    "    buf = io.BytesIO()\n",
    "    plot.figure.savefig(buf, format=\"png\")\n",
    "    plot.clf()\n",
    "    buf.seek(0)\n",
    "    blob_client = blob_service_client.get_blob_client(container='images', blob=filename.replace(\"pck\", \"png\"))\n",
    "    blob_client.upload_blob(buf, overwrite=True)\n",
    "    buf.close()\n",
    "    \n",
    "metadata = spark.read.csv(metadata_data_dir, header='true', inferSchema='true').collect()\n",
    "mris = spark.sparkContext.binaryFiles(volumetric_data_dir).collect()\n",
    "for mri in mris:\n",
    "    toImage(mri, metadata)\n",
    "print(\"FinishJob!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'etag': '\"0x1C45B376751DB40\"',\n",
       " 'last_modified': datetime.datetime(2020, 10, 13, 10, 30, 27, tzinfo=datetime.timezone.utc),\n",
       " 'client_request_id': '1c1c82ac-0d3f-11eb-9d34-0242ac120005',\n",
       " 'request_id': 'd4bfb116-f57a-4c34-b185-025a5972d259',\n",
       " 'version': '2020-02-10',\n",
       " 'date': datetime.datetime(2020, 10, 13, 10, 30, 27, tzinfo=datetime.timezone.utc),\n",
       " 'error_code': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, PublicAccess,AccessPolicy, ContainerSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "connection_string = \"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://fhirblob:10000/devstoreaccount1\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(\"images\")\n",
    "# Create access policy\n",
    "access_policy = AccessPolicy(permission=ContainerSasPermissions(read=True, write=True),\n",
    "                             expiry=datetime.utcnow() + timedelta(hours=1),\n",
    "                             start=datetime.utcnow() - timedelta(minutes=1))\n",
    "identifiers = {'read': access_policy}\n",
    "# Specifies full public read access for container and blob data.\n",
    "public_access = PublicAccess.Container\n",
    "# Set the access policy on the container\n",
    "container_client.set_container_access_policy(signed_identifiers=identifiers, public_access=public_access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
